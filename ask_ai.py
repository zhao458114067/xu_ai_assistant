import os
from dotenv import load_dotenv
from pydantic import SecretStr
from langchain.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from generate_vector_store import VECTOR_STORE_PATH

load_dotenv()
api_key = os.environ.get("API_KEY")

print("ğŸ“š æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“...")
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh",
    model_kwargs={"device": "cpu"}
)
vectorstore = FAISS.load_local(
    VECTOR_STORE_PATH, embeddings,
    allow_dangerous_deserialization=True
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

llm = ChatOpenAI(
    model="deepseek-chat",
    base_url="https://api.deepseek.com/v1",
    api_key=SecretStr(api_key),
    temperature=0.7,
    streaming=True  # å¼€å¯æµå¼æ¨¡å¼
)

print("\nğŸ¤– AI åŠ©æ‰‹å·²å¯åŠ¨ï¼Œè¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼š\n")

while True:
    try:
        query = input("ä½ é—®ï¼š")
        if query.strip().lower() in ["exit", "quit", "q"]:
            break

        docs = retriever.invoke(query)

        # æ‹¼æ¥ä¸Šä¸‹æ–‡ï¼ˆä¹Ÿå¯ä»¥æ›´å¤æ‚åœ°æ„é€  Promptï¼‰
        context = "\n\n".join([doc.page_content for doc in docs])
        full_prompt = f"åŸºäºä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ï¼š\n{context}\n\né—®é¢˜ï¼š{query}\nå›ç­”ï¼š"

        # âœ… æµå¼æ‰“å°å›ç­”
        print("å›ç­”ï¼š", end="", flush=True)
        for chunk in llm.stream(full_prompt):
            print(chunk.content, end="", flush=True)
        print("\n")

    except KeyboardInterrupt:
        break
